# -*- coding: utf-8 -*-
"""Intelligent_Admissions.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/136fGeWHlg3kTlyFJOxoXvVhJBUQuK90c
"""

# Commented out IPython magic to ensure Python compatibility.
from mpl_toolkits.mplot3d import Axes3D
from sklearn.preprocessing import StandardScaler
import matplotlib.pyplot as plt # plotting
import numpy as np # linear algebra
import os # accessing directory structure
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)
import seaborn as sns
# %matplotlib inline
from  google.colab import files
import io

data=files.upload()

data=pd.read_csv('/content/Admission_Predict.csv')

data.info()

data.isnull().any()

data=data.rename(columns ={'Chance of Admit ': 'Chance of Admit','LOR ':'LOR'})

data.info()

data.describe()

sns.distplot(data["GRE Score"])

ax = sns.heatmap(data.corr(), annot=False)

sns.pairplot(data=data, hue="Research", markers=["^", "v"],palette="inferno")

sns.scatterplot(x="University Rating",y="CGPA",data=data,color="Red",s=100)

data.info()

category=["GRE Score",'TOEFL Score','University Rating','SOP','LOR','CGPA','Research','Chance of Admit']
color=["yellowgreen","gold","lightskyblue",'pink','red','purple','orange','grey']
start=True
for i in np.arange(4):
  fig=plt.figure(figsize=(14,8))
  plt.subplot2grid((4,2),(i,0))
  data[category[2*i]].hist(color=color[2*i],bins=10)
  plt.title(category[2*i])
  plt.subplot2grid((4,2),(i,1))
  data[category[2*i+1]].hist(color=color[2*i+1],bins=10)
  plt.title(category[2*i+1])

plt.subplots_adjust(hspace=0.7,wspace=0.2)
plt.show()

x=data.iloc[:,0:7].values
x

y=data.iloc[:,7].values
y

from sklearn.preprocessing import MinMaxScaler
sc=MinMaxScaler()
x_sc=sc.fit_transform(x)
x_sc

from sklearn.model_selection import train_test_split
x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.3,random_state=101)

y_train=(y_train>0.5)
y_train

y_test=(y_test>0.5)
y_test

from sklearn.linear_model import LogisticRegression
lr=LogisticRegression(random_state=0)
l=lr.fit(x_train,y_train)
y_pred=lr.predict(x_test)
l
y_pred

import tensorflow as tf
from tensorflow import keras
from tensorflow.keras.layers import Dense, Activation,Dropout

classifier=keras.Sequential()
classifier.add(Dense(7,activation='relu',input_dim=7))

classifier.add(Dense(7,activation='relu'))
classifier.add(Dense(7,activation='linear'))

classifier.summary()

loss_1=tf.keras.losses.BinaryCrossentropy()
classifier.compile(loss=loss_1,optimizer="Adam",metrics=['accuracy'])
classifier.fit(x_train,y_train,batch_size=20,epochs=100)

from sklearn.metrics import accuracy_score
t_p=classifier.predict(x_train)
print(t_p)

tr_acc=classifier.evaluate(x_train,y_train,verbose=0)[1]
print(tr_acc)

test_acc=classifier.evaluate(x_test,y_test,verbose=0)[1]
print(test_acc)

from sklearn.metrics import accuracy_score,recall_score, roc_auc_score, confusion_matrix,classification_report
print("Accuracy Score : %f" %(accuracy_score(y_test,y_pred)*100))
print("Recall Score : %f" %(recall_score(y_test,y_pred)*100))
print("ROC Score : %f" %(roc_auc_score(y_test,y_pred)*100))
print("\nConfusion Matrix")
print(confusion_matrix(y_test,y_pred))
print("\nClassification Report")
print(classification_report(y_test,y_pred))

classifier.save("IAdmission.h5")